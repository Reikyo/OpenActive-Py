<img src='images/OpenActive-Landscape-Logo-2.png' width='500'>

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/Reikyo/OpenActive-Py/main?labpath=index.ipynb)

# OpenActive-Py
This repository contains files for exploring OpenActive data via Python. The main content can be found in the `app.py` file and the `cache/` directory, with the latter containing files generated by the former. There are some starter files there to quickly begin exploring the data, but it is possible and encouraged to refresh these files to get the latest information for your own use when running elsewhere. All other files herein are simply setup files for running in various ways in various environments. Explicitly, we have:

For all running options:
- cache/
- app.py

For running locally:
- requirements.txt

For running on Binder:
- environment.yml
- index.ipynb

For running on Heroku:
- Procfile
- requirements.txt
- runtime.txt

See the `index.ipynb` Jupyter notebook for an overview of the functionality of `app.py` and the content of its outputs. If you wish to explore this further without installing anything, simply click the 'launch Binder' badge beneath the OpenActive logo at the top of this page, and a virtual machine running this notebook will be prepared for you within a few minutes. You can then run the cells by pressing Shift-Enter on each one, and add your own Python code in further cells to explore the data as you like. Note that the virtual machine will stop running after a few minutes of inactivity and cannot be restarted, you will have to begin again from scratch, so be sure to note any useful code that you generate if you do intend to leave the service idle for a while.

## Running locally

To run locally, first clone this repository to a destination of your choice, and ensure that you have the contents of `requirements.txt` installed too. You can then import the contents of `app.py` as seen in the `index.ipynb` example.

Alternatively, you can run the code as a Flask microservice, and `app.py` is already set up for that purpose. First make sure you have the Python `virtualenv` package installed, then make your own virtual environment with the contents of `requirements.txt` and run the programme as follows:

```
$ virtualenv virt
$ source virt/bin/activate
(virt) $ pip install -r requirements.txt
(virt) $ python app.py
    * Serving Flask app 'app'
    * Debug mode: off
    WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
    * Running on http://127.0.0.1:5000
```

You can then access the endpoints via a web browser or a tool like [Postman](https://www.postman.com/). The endpoints are all named the same as the functions that they run, so to run a function called `funky` you would visit `http://127.0.0.1:5000/funky`, and so on. The `funky` function does not actually exist, but the functions that do exist are all equally funky, and are explained below.

## Functions

The OpenActive functions within `app.py` are:

- `get_catalogue_urls`
- `get_dataset_urls`
- `get_feed_urls`
- `get_feeds`
- `get_opportunities`

If you are mainly interested in the OpenActive opportunity data, then you can just use `get_opportunities` and ignore the rest, which will be automatically called in turn if cache refresh is requested, as explained below.

By default, the data at any stage is output in a nested structure that shows how it was obtained. Each opportunity belongs to a particular feed, which belongs to a particular dataset, which belongs to a particular catalogue. So, for example, the opportunity data has the following structure:

```
{
    "https://opendata.leisurecloud.live/api/datacatalog": {
        "https://api.activenewham.org.uk/OpenActive/": {
            "https://opendata.leisurecloud.live/api/feeds/ActiveNewham-live-live-session-series": [
                {
                    "id": "B2CLJNR16000816",
                    "kind": "SessionSeries",
                    "name": "Junior Gym Tues 4pm",
                    "latitude": 51.523460797563594,
                    "longitude": 0.02305090427398682
                },
                ...
```

Other data from earlier in the data gathering chain, such as the feeds of opportunities and the datasets of feeds, naturally have a shallower overall structure. For any function, the output can be either flattened to show only the terminal lists all tied together as one long list, or expanded to show metadata that includes the sub-list counts and time of last refresh. It is this latter form which is actually present in the variables passed between functions behind the scenes, and in the cached files too.

To get the flattened form, use the `doFlatten` keyword argument and set to `True`. So in a local Python environment with `app.py` imported as `oa` (as seen in `index.ipynb`) you would do:

```
oa.get_opportunities(doFlatten=True)
```

And if running as a Flask microservice, then you would visit the following in the browser:

```
http://127.0.0.1:5000/get_opportunities?doFlatten=True
```

To get the metadata form, use the `doMetadata` keyword argument and set to `True`, just like the above. Note that if both `doFlatten` and `doMetadata` are set to `True`, then the former takes precedence.

Finally, use the `doRefresh` keyword argument and set to `True` in order to refresh the cached files for the function in question and all those before it in the data gathering chain. For example, if we refresh the `get_dataset_urls` function, then both the catalogue URLs and the dataset URLs will be refreshed, but not the feed data or the opportunity data. But if we refresh the `get_opportunities` function, then all cached data will be refreshed, as this sits at the very end of the chain. The more of the data chain that is refreshed, then the longer it will take, up to a few minutes in the case of `get_opportunities` seeing as it requires the most work.
